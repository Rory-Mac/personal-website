<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Digital Neural Networks</title>
</head>
<body>
    <header class="site-header">
        <div class="wrapper">
            <a class="site-title hover-animation" href="../index.html">RoryMac.blog </a>
            <nav class="site-nav">
                <a class="nav-link hover-animation" href="../index.html">About</a>
                <a class="nav-link hover-animation" href="../explore.html">Explore</a>
                <a class="nav-link hover-animation" href="../projects.html">Projects</a>
                <a class="nav-link hover-animation" href="../blog.html">Blog</a>
            </nav>
        </div>
        <hr>
    </header>
    <main class="page-content">
        <div class="wrapper">
            <h3>Digital Neural Networks</h3>
            <p> Let's first consider the structure of a single digital neuron:</p>
            <p><img src="../Assets/images/digital_neuron2.png" width="100%" height="100%"></p>
            <p>Each activation in a set of input nodes is weighted, then all activations are summed, a bias is applied to the result and then converted into a boolean
            value that decides whether the perceptron activates or not. Perceptrons are layered to form a neural network:</p>
            <p><img src="../Assets/images/digital_neurons.png" width="100%" height="100%"></p>
            The mathematical expression for determining the activation of a perceptron is as follows:
                \[ σ\left(\left(\sum_{i=1}^n a_iW_i\right) + b \right) \]
            This scales to a layer of neurons like so:
            <p>
                \[
                σ\left(
                \begin{bmatrix} W_{0,0} & W_{1,0} & \dots & W_{n,0} \\ W_{0,1} & W_{1,1} & \dots & W_{n,1} \\ 
                \vdots & \vdots & \ddots & \vdots \\ W_{0,n} & W_{1,n} & \dots & W_{n,n} \end{bmatrix}
                \begin{bmatrix} a_0 \\ a_1 \\ \vdots \\ a_n \end{bmatrix} +  
                \begin{bmatrix} b_0 \\ b_1 \\ \vdots \\ b_n \end{bmatrix}
                \right) = \begin{bmatrix} o_0 \\ o_1 \\ \vdots \\ o_n \end{bmatrix}
                \]
            </p>
            <p> \( a_1..a_n \> \) represents the activation values of the input neurons. \( o_1..o_n\) represents the activation values of the output neurons. 
                The activation function (denoted σ above) is a function that scales the range of possible activation in each neuron. The sigmoid function is a
                mathematical function that performs exactly this: 
                \[ σ(x) = \frac{1}{1+e^{-x}} \]</p>
            <p>
                ReLU (rectified Linear Unit) is a popular alternative that imitates the boolean nature of activation in biological neurons:
                \[ σ(x) = max(0, x) \]
            </p>
            <p> There exist many activation functions including the step function, signum function, linear function, leaky ReLU, 
                Hyperbolic tangent function and softmax function. These all modify the range of activation a neuron possesses. As a general rule, the more limited
                the set of possible values the activation function produces, the faster the speed of computation in the neural network.</p>
            <p> A loss/reward function is a function that maps a set of variables to a variable representing a cost/reward. An optimisation problem is a problem with 
                the goal of determining the minimum/maximum, minimum in the case of a cost function, maximum in the case of a reward function.  
                In artificial intelligence, the loss/reward function incentivises the neural network to train towards a given objective.</p>
                
        </div>
    </main>
</body>
</html>