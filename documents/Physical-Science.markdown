---
layout: narrow
title: Physical-Science
---
<h4>Physical Science</h4>
<p>Quantum Field Theory is a theoretical framework for describing the universe and exists as the reconciliation of special relativity
with quantum mechanics. Special relativity is a means of describing observed relationships between speed, mass, time and space. Quantum
mechanics is a means of resolving wave-particle duality: the observation that quantum entities act as both particles and waves under 
different experimental circumstances. Quantum Field Theory fails to account for general relativity's description of gravity as the 
curvature of space-time. Reconciliation of the two would provide a theory of everything: of the fabric of the universe, of matter, energy,
space and time.</p>
<p>To provide an overview, quantum field theory asserts the existence of a set of quantum fields corresponding to particles in the standard
model. Particles are excitations in their respective quantum field. Quantum fields can be thought of as mathematical constructs that 
attribute a mathematical object to every point in space-time. Special relativity asserts space-time to have certain symmetries of 
time, space, rotation and change of frame of reference. If the universe is a closed system, then these symmetries enforce laws of
conservation. If the rules of the system are time-invariant, then net energy is conserved across time. If the rules of the system are 
space-invariant, then net momentum is conserved across time. If the rules of the system are invariant under rotation, then angular momentum 
is conserved across time. Finally, if momentum is constant, and the rules of the system are invariant under boosts (changes to the inertial 
frame of reference), then velocity of the centre of mass is constant in the absence of external forces as well. Additional symmetries 
introduce additional conservation laws. These are symmetries belonging to the mathematical objects that comprise our quantum fields.</p>
<p>The standard model of particle physics results from the classification of particles with respect to the internal symmetries of their 
respective quantum fields. They are quantum fields in the sense that like quantum particles, the fields themselves exist in a superposition
of possible values, as do their excitations, that interact with one another. The fundamental forces of the universe can be described as 
the result of said interactions between quantum fields, that is except gravity. String theory exists as the most developed framework for 
reconciling general relativity and the standard model.</p>
<p>The particles of the standard model can be subdivided into bosons (force-carriers) and fermions (matter particles). Bosons include
the Higgs field which gives mass to other subatomic particles, the gluon responsible for the strong nuclear force, the Z and W bosons
responsible for the weak nuclear force, and the photon, responsible for the electromagnetic force. Fermions include electrons and 
nuclei-forming up and down quarks, Muons and Tau paticles that are heavy and decay rapidly into electrons and neutrinos, neutrinos
that are extremely light and have little interaction with other matter, and other quarks (charm, strange, top and bottom quarks) that are
heavy and decay into up and down quarks. The chemical study of our universe mostly concerns itself with the less exotic, more stable
forms of matter we observe in the universe's current state.</p>
<p>Thus we can rest the periodic table of elements as a heuristic atop the standard model of particle physics, and visualise both as 
commutative diagrams depicting entities and their relations. We can visualise the flattening of the periodic table over an x axis, extend
it to include ions and isotopes, and construct chemical products of reactants on the y axis, with the product's height in the y axis
corresponding to the length of the shortest path from its primitive constituents (elements) to itself. This height value will also
approximate the structural complexity of the chemical product. We can imagine the universe as a traversal within this diagram: determine
the occurence of each entity (chemical elements and compounds) and each relation (chemical reaction) in the universe's current state. 
Reactions are reversible or  irreversible, equivalent to uni-directional or bi-directional relations. We could zoom in on a local region of
the universe to provide a similar traversal that is more descriptive to that local region. However, the presence of these components alone
is not enough to determine the nature of this traversal. There are conditions that causally relate to the occurence of a reaction. This 
includes the phase of the reactants, determing the surface area of the interface over which chemical reaction could take place. It includes
temperature and pressure (kinetic energy per particle / per unit volume respectively). It includes the density and the concentration of 
the reactants (amount relative to other present chemical species / amount per unit volume) as well as the presence or absence of catalysts.
These conditions are themselves causally linked, for instance temperature/pressure may determine phase may determine activation energy, etc.</p>
<p>We want to be able to describe these properties of the physical universe from any point within the physical universe, however. 
Thus, we require a set of base units that rest upon constants that are space-time invariant. We can 
define the second t (the base unit of time) as the duration of 9192631770 periods of the radiation emitted by a caesium-133 atom's transition
between hyperfine levels at ground state. We can define the metre (the base unit of length), as one 299792458th the length travelled by
light in a vacuum in one second. Thus we have a space-time invariant definition of the units of space-time. The large arbitrary values 
in these relations result from the conversion of earlier to newer definitions (period of rotation of earth, and percentage of a geodesic on
earth, respectively). These definitions along with the planck constant (the ratio of a photon's energy to its frequency) are used to 
define the unit of mass in kilograms. The kelvin (the base unit of thermodynamic temperature) is defined using all these definitions along
with the boltzmann constant k (the proportionality factor of thermal energy per particle in a gas to thermodynamic temperature of said gas).
The ampere (the base unit of electric current) is defined using the elementary charge e. The mole (the base unit of amount of substance) is
defined using the avagadro constant (defined as the number of atoms in 12g of carbon-12). And finally, the candela (the base unit of 
luminous intensity) is defined using the definition of the joule, the second, and the constant of luminous efficacy at 540 THz. Derived
units can be derived by taking the product of powers of base units, providing a less abstract, more familiar zoo of physical measurements.</p>
<p><img src="/Assets/images/carnot_engine.png" width="100%" height="100%"></p>
<p>In order for any aspect of this traversal to proceed, energy is required. Energy can be thought of as the capacity to do work, and work
as the transference of energy through force causing displacement. Energy can be kinetic (existing due to the motion of an object) or 
potential (existing due to an object's position relative to other objects). Other forms of energy, thermal, chemical, nuclear, etc. are 
manifestations of these abstract forms of energy. We can imagine an idealised system that maximises the conversion of thermal energy to
work via the Carnot Heat Cycle. We imagine an ideal gas situated within a piston-sealed chamber. Beside this chamber exist two unlimited
thermal reservoirs, one relatively hotter, and one relatively cooler. The heat cycle consists of four steps. First, an infinitisemal 
quantity of thermal energy is transferred from the hot reservoir to the gas, causing a proportional quantity of work to be exerted against
the piston with an equivalent increase in volume and decrease in pressure. Next, the gas continues to expand in the absence of the hot
reservoir, causing a decrease in temperature, until it is an infinitesimal degree warmer than the cold reservoir. At this point, the cold
reservoir comes in contact with the gas and acts as a heat sink for thermal energy transferred from the piston's reciprocal action as it 
exerts work on the gas, causing increase pressure and decreased volume. Next, this compression continues in the absence of the cold reservoir, causing an increase in temperature, until the gas is an 
infinitesimal degree colder than the hot reservoir: our initial state. This process is said to be infinitely slow and thus reversible. 
We could similarly use the cold reservoir as the heat source and the hot reservoir as the heat sink by ensuring that the gas in its 
boundary conditions are slightly colder than the cold reservoir and slightly warmer than the hot reservoir.</p>
<p><img src="/Assets/images/carnot_cycle.png" width="100%" height="100%"></p>
<p>Entropy is an additional property conserved within this system, changing with respect to the thermal differential between gas and
reservoir at the point of contact. It can be thought of as the tendency for energy to distribute itself across the system. A more 
accurate view of entropy starts with the notion of multiplicity. We imagine N equally probable micro-states over which q units of energy
can be distributed. Multiplicity is given as the following combination, which can be visualised as every way N - 1 boundaries dividing q
elements can be manipulated.
\[ \Omega (N, q) = \left( \begin{array}{c} q + N - 1 \\ q \end{array} \right) = \frac{q+N-1!}{q!(N-1)} \]
The macro-state is characterized by a set of macroscopic properties. The macro-state exists as the set of micro-states that could produce 
these macroscopic properties. The following are the Boltzmann and Gibbs definitions of entropy respectively.
\[ S = k_{B} ​\ln Ω \]
\[ S = −k_{B} ​i \Sigma p_i ​\ln p_i​ \]
In Gibbs entropy, entropy is maximised when all states are equally probable, and minimised when one state is maximally probable. It is 
a generalisation of Boltzmann's entropy, which can be derived by setting \(p_i = 1/\Omega\) for \(\Omega\) micro-states. Gibbs Free
Energy uses these definitions to determine usable energy within a given system.
\[ G(p,t) = U + pV - TS = H - TS \]
\[ \delta G = \delta H - T \delta S \]
The change in Gibbs Free Energy is the maximum amount of non-volume expansion work that can performed as a result of a change in enthalpy
within a closed system. Shannon's entropy is formally equivalent to Gibbs entropy but exist within the context of information theory. Thus
parallels can be drawn between heuristics of entropy in the physical universe and in the context of information theory. In information 
theory, the degree to which information is low entropy is equivalent to the degree to which it can be compressed. Thus, biological entities 
with nervous systems can be thought of as systems that increase entropy through physiological function to minimise the entropy of stored 
information.</p>
<p>There seem to also exist relationships between entropy and complexity that are not so easily articulated. A system can be said to 
be complex if its macroscopic properties cannot be inferred from its microscopic properties except through simulation, even if both are
known. Examples include the n-body problem, Conway's game of life, economonic trade, and meteorology. These systems can emerge from the
transformation of low to high entropy systems.</p>